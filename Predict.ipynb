{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from os import path\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import spacy\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_token(review):\n",
    "\n",
    "  # drop points like . . . . . . . . . ., and not useful characters. \n",
    "  tokenizer = RegexpTokenizer(\"\\w+\\'?\\w+|\\w+\") \n",
    "  \n",
    "  return tokenizer.tokenize(str(review))\n",
    "\n",
    "def remove_stopwords(review):\n",
    "\n",
    "    #load stop words \n",
    "    stop_words = stopwords.words('english')\n",
    "    exceptionStopWords = {\n",
    "      'again',\n",
    "      'against',\n",
    "      'ain',\n",
    "      'almost',\n",
    "      'among',\n",
    "      'amongst',\n",
    "      'amount',\n",
    "      'anyhow',\n",
    "      'anyway',\n",
    "      'aren',\n",
    "      \"aren't\",\n",
    "      'below',\n",
    "      'bottom',\n",
    "      'but',\n",
    "      'cannot',\n",
    "      'couldn',\n",
    "      \"couldn't\",\n",
    "      'didn',\n",
    "      \"didn't\",\n",
    "      'doesn',\n",
    "      \"doesn't\",\n",
    "      'don',\n",
    "      \"don't\",\n",
    "      'done',\n",
    "      'down',\n",
    "      'except',\n",
    "      'few',\n",
    "      'hadn',\n",
    "      \"hadn't\",\n",
    "      'hasn',\n",
    "      \"hasn't\",\n",
    "      'haven',\n",
    "      \"haven't\",\n",
    "      'however',\n",
    "      'isn',\n",
    "      \"isn't\",\n",
    "      'least',\n",
    "      'mightn',\n",
    "      \"mightn't\",\n",
    "      'move',\n",
    "      'much',\n",
    "      'must',\n",
    "      'mustn',\n",
    "      \"mustn't\",\n",
    "      'needn',\n",
    "      \"needn't\",\n",
    "      'neither',\n",
    "      'never',\n",
    "      'nevertheless',\n",
    "      'no',\n",
    "      'nobody',\n",
    "      'none',\n",
    "      'noone',\n",
    "      'nor',\n",
    "      'not',\n",
    "      'nothing',\n",
    "      'should',\n",
    "      \"should've\",\n",
    "      'shouldn',\n",
    "      \"shouldn't\",\n",
    "      'too',\n",
    "      'top',\n",
    "      'up',\n",
    "      'very'\n",
    "      'wasn',\n",
    "      \"wasn't\",\n",
    "      'well',\n",
    "      'weren',\n",
    "      \"weren't\",\n",
    "      'won',\n",
    "      \"won't\",\n",
    "      'wouldn',\n",
    "      \"wouldn't\"\n",
    "}\n",
    "\n",
    "    # union and clean basic stop words\n",
    "    stop_words = set(stop_words).union(STOP_WORDS)\n",
    "    final_stop_words = stop_words-exceptionStopWords\n",
    "\n",
    "    return [token for token in review if token not in final_stop_words]\n",
    "\n",
    "def lemmatization(review):\n",
    "    \n",
    "    # Part-of-speech tagging. When you switch off -  disable - parser, tagger, ner it could work more faster \n",
    "    nlp = spacy.load(\"en\",disable=['parser', 'tagger', 'ner']) \n",
    "    lemma_result = []\n",
    "    \n",
    "    for words in review:\n",
    "        doc = nlp(words)\n",
    "        for token in doc:\n",
    "            lemma_result.append(token.lemma_)\n",
    "    return lemma_result\n",
    "\n",
    "\n",
    "def pipeline(review):\n",
    "    review = make_token(review)\n",
    "    review = remove_stopwords(review)\n",
    "    return lemmatization(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout,embedding_weights):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_weights)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout)\n",
    "        # если было бы три леира тогда умножили на 3,  так у нас два (N_LAYERS = 2)\n",
    "        self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, text_lengths):\n",
    "        #x [sent length , batch size]\n",
    "        embedded = self.embedding(x) #[sentect len,batch size,embedding dim]\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths) # это сделано для того, что бы выровнять матрицу ввиду того, что длина банчей у каждого разная. https://stackoverflow.com/questions/51030782/why-do-we-pack-the-sequences-in-pytorch\n",
    "        packed_output, (hidden, cell) = self.rnn(packed_embedded)#output[sent length,batch size,hiddendin*num of directions],[numberlayers*num of dir,batch size,hid dim]\n",
    "        #[f0,b0,f1,b1,.......fn,bn]\n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)) # кантакатинация выхода от двух hidden N_LAYERS \n",
    "        return self.fc(hidden.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2idx(embedding_model,review):\n",
    "    index_review = []\n",
    "    for word in review:\n",
    "        try:\n",
    "            index_review.append(embedding_model.vocab[word].index)\n",
    "        except: \n",
    "             pass\n",
    "    return torch.tensor(index_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(sentence, word_vectors):\n",
    "    tokenized = pipeline(sentence)\n",
    "    indexed = word2idx(word_vectors,tokenized)\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    prediction = torch.sigmoid(the_model(tensor,torch.LongTensor([len(indexed)]).to(device)))\n",
    "    return prediction.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/artem/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_W2V = '/home/artem/pr_sissi_sentiment_model_google-cloud-functions/models'\n",
    "PATH_model = '/home/artem/pr_sissi_sentiment_model_google-cloud-functions/models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = KeyedVectors.load(path.join(PATH_W2V,'word2vec.model'))\n",
    "embedding_weights = torch.Tensor(word_vectors.vectors)\n",
    "padding_value = len(word_vectors.index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = padding_value\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "the_model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT, embedding_weights).to(device)\n",
    "the_model.load_state_dict(torch.load(path.join(PATH_model,'sissi_sentiment_12122019.pth'),  map_location='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7459672093391418"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(\"comedy movie\", word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
